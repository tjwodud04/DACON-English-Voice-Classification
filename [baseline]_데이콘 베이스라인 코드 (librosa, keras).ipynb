{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import librosa\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "africa_train_paths = glob(\"./open/train/africa/*.wav\")\n",
    "australia_train_paths = glob(\"./open/train/australia/*.wav\")\n",
    "canada_train_paths = glob(\"./open/train/canada/*.wav\")\n",
    "england_train_paths = glob(\"./open/train/england/*.wav\")\n",
    "hongkong_train_paths = glob(\"./open/train/hongkong/*.wav\")\n",
    "us_train_paths = glob(\"./open/train/us/*.wav\")\n",
    "\n",
    "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
    "             england_train_paths, hongkong_train_paths, us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./open/test\\1.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./open/test\\10.wav</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./open/test\\100.wav</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./open/test\\1000.wav</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./open/test\\1001.wav</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   path    id\n",
       "0     ./open/test\\1.wav     1\n",
       "1    ./open/test\\10.wav    10\n",
       "2   ./open/test\\100.wav   100\n",
       "3  ./open/test\\1000.wav  1000\n",
       "4  ./open/test\\1001.wav  1001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n",
    "# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n",
    "\n",
    "def get_id(data):\n",
    "    return np.int(data.split(\"\\\\\")[1].split(\".\")[0])\n",
    "\n",
    "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
    "test_[\"path\"] = glob(\"./open/test/*.wav\")\n",
    "test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n",
    "\n",
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline 코드에서는 librosa 라이브러리를 사용하여 wav파일을 전처리 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result) \n",
    "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
    "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
    "\n",
    "os.mkdir(\"./npy_data\")\n",
    "\n",
    "africa_train_data = load_data(africa_train_paths)\n",
    "np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "np.save(\"./npy_data/england_npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "np.save(\"./npy_data/us_npy\", us_train_data)\n",
    "\n",
    "test_data = load_data(test_[\"path\"])\n",
    "np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy파일로 저장된 데이터를 불러옵니다.\n",
    "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n",
    "australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "canada_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n",
    "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
    "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n",
    "\n",
    "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
    "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
    "\n",
    "def get_mini(data):\n",
    "\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "#음성들의 길이를 맞춰줍니다.\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "#feature를 생성합니다.\n",
    "\n",
    "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
    "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "\n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis= 0)\n",
    "test_x = np.array(test_data)\n",
    "\n",
    "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
    "\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
    "\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)\n",
    "\n",
    "# librosa를 이용해 feature를 추출합니다.\n",
    "\n",
    "train_x = get_feature(data = train_x)\n",
    "test_x = get_feature(data = test_x)\n",
    "\n",
    "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data의 label을 생성해 줍니다.\n",
    "\n",
    "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
    "                        np.ones(len(australia_train_data), dtype = np.int),\n",
    "                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n",
    "                         np.ones(len(england_train_data), dtype = np.int) * 3,\n",
    "                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n",
    "                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25520, 64, 501, 1), (25520,), (6100, 64, 501, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 분석 모델은 월간데이콘_6 음성 중첩 데이터 분류 AI 경진대회 3위를 달성하신 Jamm님의 코드를 바탕으로 만들어졌습니다.  \n",
    " https://www.dacon.io/competitions/official/235616/codeshare/1571?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
    "                                     Dropout, Dense, AveragePooling2D, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input_, units = 32, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
    "    \n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn():\n",
    "    dropout_rate = 0.3\n",
    "    \n",
    "    in_ = Input(shape = (train_x.shape[1:]))\n",
    "    \n",
    "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
    "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
    "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
    "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
    "\n",
    "    x = Flatten()(block_05)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    x = Dense(units = 128, activation = \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_res, x])\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "\n",
    "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
    "    model = Model(in_, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "638/638 [==============================] - 52s 65ms/step - loss: 1.8038 - acc: 0.3647 - val_loss: 1.2942 - val_acc: 0.4596\n",
      "Epoch 2/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.3224 - acc: 0.4523 - val_loss: 1.2439 - val_acc: 0.4869\n",
      "Epoch 3/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.2478 - acc: 0.4950 - val_loss: 1.2700 - val_acc: 0.4941\n",
      "Epoch 4/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.1910 - acc: 0.5236 - val_loss: 1.1862 - val_acc: 0.5421\n",
      "Epoch 5/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.1202 - acc: 0.5715 - val_loss: 1.3597 - val_acc: 0.4879\n",
      "Epoch 6/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.0261 - acc: 0.6075 - val_loss: 1.2361 - val_acc: 0.5498\n",
      "Epoch 7/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 0.9725 - acc: 0.6322 - val_loss: 0.9856 - val_acc: 0.6268\n",
      "Epoch 8/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 0.9459 - acc: 0.6422 - val_loss: 1.1350 - val_acc: 0.5841\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "638/638 [==============================] - 43s 64ms/step - loss: 1.7700 - acc: 0.3590 - val_loss: 1.2983 - val_acc: 0.4667\n",
      "Epoch 2/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.3009 - acc: 0.4671 - val_loss: 1.3157 - val_acc: 0.4628\n",
      "Epoch 3/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.1976 - acc: 0.5318 - val_loss: 1.2051 - val_acc: 0.5133\n",
      "Epoch 4/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.1312 - acc: 0.5522 - val_loss: 1.3431 - val_acc: 0.4875\n",
      "Epoch 5/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.0789 - acc: 0.5833 - val_loss: 1.3431 - val_acc: 0.5053\n",
      "Epoch 6/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.0453 - acc: 0.5954 - val_loss: 1.4556 - val_acc: 0.4422\n",
      "Epoch 7/8\n",
      "638/638 [==============================] - 41s 63ms/step - loss: 0.9818 - acc: 0.6282 - val_loss: 1.0469 - val_acc: 0.6215\n",
      "Epoch 8/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 0.9043 - acc: 0.6631 - val_loss: 1.3633 - val_acc: 0.5206\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "638/638 [==============================] - 43s 64ms/step - loss: 1.8475 - acc: 0.3567 - val_loss: 1.3766 - val_acc: 0.3999\n",
      "Epoch 2/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 1.3144 - acc: 0.4423 - val_loss: 1.3188 - val_acc: 0.4594\n",
      "Epoch 3/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 1.2455 - acc: 0.4876 - val_loss: 1.2905 - val_acc: 0.4937\n",
      "Epoch 4/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 1.1748 - acc: 0.5432 - val_loss: 1.9004 - val_acc: 0.2351\n",
      "Epoch 5/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.1113 - acc: 0.5783 - val_loss: 1.2524 - val_acc: 0.5098\n",
      "Epoch 6/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 1.0512 - acc: 0.6075 - val_loss: 1.2876 - val_acc: 0.5315\n",
      "Epoch 7/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 0.9920 - acc: 0.6275 - val_loss: 1.1646 - val_acc: 0.5562\n",
      "Epoch 8/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 0.9338 - acc: 0.6500 - val_loss: 1.0360 - val_acc: 0.5946\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "638/638 [==============================] - 43s 64ms/step - loss: 1.8397 - acc: 0.3622 - val_loss: 1.4138 - val_acc: 0.4389\n",
      "Epoch 2/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 1.2973 - acc: 0.4661 - val_loss: 1.4423 - val_acc: 0.4018\n",
      "Epoch 3/8\n",
      "638/638 [==============================] - 40s 62ms/step - loss: 1.2071 - acc: 0.5212 - val_loss: 1.1864 - val_acc: 0.5558\n",
      "Epoch 4/8\n",
      "638/638 [==============================] - 38s 60ms/step - loss: 1.1077 - acc: 0.5816 - val_loss: 1.3264 - val_acc: 0.5039\n",
      "Epoch 5/8\n",
      "638/638 [==============================] - 38s 60ms/step - loss: 1.0553 - acc: 0.5998 - val_loss: 1.5397 - val_acc: 0.5088\n",
      "Epoch 6/8\n",
      "638/638 [==============================] - 38s 60ms/step - loss: 0.9753 - acc: 0.6369 - val_loss: 1.1772 - val_acc: 0.5735\n",
      "Epoch 7/8\n",
      "638/638 [==============================] - 40s 63ms/step - loss: 0.9166 - acc: 0.6469 - val_loss: 1.1530 - val_acc: 0.6056\n",
      "Epoch 8/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 0.8631 - acc: 0.6734 - val_loss: 1.0187 - val_acc: 0.6103\n",
      "*******************************************************************\n",
      "*******************************************************************\n",
      "Epoch 1/8\n",
      "638/638 [==============================] - 43s 65ms/step - loss: 1.8114 - acc: 0.3531 - val_loss: 1.3179 - val_acc: 0.4551\n",
      "Epoch 2/8\n",
      "638/638 [==============================] - 41s 64ms/step - loss: 1.3114 - acc: 0.4584 - val_loss: 1.3706 - val_acc: 0.4414\n",
      "Epoch 3/8\n",
      "638/638 [==============================] - 40s 62ms/step - loss: 1.2119 - acc: 0.5176 - val_loss: 1.3321 - val_acc: 0.4902\n",
      "Epoch 4/8\n",
      "638/638 [==============================] - 39s 62ms/step - loss: 1.1678 - acc: 0.5425 - val_loss: 2.0583 - val_acc: 0.3007\n",
      "Epoch 5/8\n",
      "638/638 [==============================] - 43s 67ms/step - loss: 1.2306 - acc: 0.5091 - val_loss: 1.7372 - val_acc: 0.4208\n",
      "Epoch 6/8\n",
      "638/638 [==============================] - 44s 69ms/step - loss: 1.1141 - acc: 0.5729 - val_loss: 1.6575 - val_acc: 0.4095\n",
      "Epoch 7/8\n",
      "638/638 [==============================] - 44s 68ms/step - loss: 1.0122 - acc: 0.6164 - val_loss: 1.1436 - val_acc: 0.5856\n",
      "Epoch 8/8\n",
      "638/638 [==============================] - 44s 68ms/step - loss: 0.9615 - acc: 0.6364 - val_loss: 1.1160 - val_acc: 0.6007\n",
      "*******************************************************************\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n",
    "\n",
    "pred = []\n",
    "pred_ = []\n",
    "\n",
    "for train_idx, val_idx in split.split(train_x, train_y):\n",
    "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = build_fn()\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.002),\n",
    "                 loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics = ['acc'])\n",
    "\n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 8)\n",
    "    print(\"*******************************************************************\")\n",
    "    pred.append(model.predict(test_x))\n",
    "    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n",
    "    print(\"*******************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
    "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
    "\n",
    "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission[\"id\"], result)\n",
    "result.columns = sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>africa</th>\n",
       "      <th>australia</th>\n",
       "      <th>canada</th>\n",
       "      <th>england</th>\n",
       "      <th>hongkong</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.138464</td>\n",
       "      <td>0.033938</td>\n",
       "      <td>0.038915</td>\n",
       "      <td>0.199755</td>\n",
       "      <td>0.090676</td>\n",
       "      <td>0.498252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.032724</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.026849</td>\n",
       "      <td>0.384021</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.524257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.103079</td>\n",
       "      <td>0.017599</td>\n",
       "      <td>0.019485</td>\n",
       "      <td>0.668965</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.185944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>0.052093</td>\n",
       "      <td>0.047802</td>\n",
       "      <td>0.536455</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.306082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.147494</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>0.806062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>6096</td>\n",
       "      <td>0.013074</td>\n",
       "      <td>0.054897</td>\n",
       "      <td>0.091971</td>\n",
       "      <td>0.340647</td>\n",
       "      <td>0.291324</td>\n",
       "      <td>0.208088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>6097</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.190241</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.781260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>6098</td>\n",
       "      <td>0.074573</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.566452</td>\n",
       "      <td>0.046567</td>\n",
       "      <td>0.266950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>6099</td>\n",
       "      <td>0.107867</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.380903</td>\n",
       "      <td>0.058260</td>\n",
       "      <td>0.426176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>6100</td>\n",
       "      <td>0.082555</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.557220</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.317345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    africa  australia    canada   england  hongkong        us\n",
       "0        1  0.138464   0.033938  0.038915  0.199755  0.090676  0.498252\n",
       "1        2  0.032724   0.025368  0.026849  0.384021  0.006782  0.524257\n",
       "2        3  0.103079   0.017599  0.019485  0.668965  0.004928  0.185944\n",
       "3        4  0.049161   0.052093  0.047802  0.536455  0.008407  0.306082\n",
       "4        5  0.015329   0.004383  0.007876  0.147494  0.018856  0.806062\n",
       "...    ...       ...        ...       ...       ...       ...       ...\n",
       "6095  6096  0.013074   0.054897  0.091971  0.340647  0.291324  0.208088\n",
       "6096  6097  0.009162   0.004193  0.006484  0.190241  0.008660  0.781260\n",
       "6097  6098  0.074573   0.024427  0.021031  0.566452  0.046567  0.266950\n",
       "6098  6099  0.107867   0.008260  0.018533  0.380903  0.058260  0.426176\n",
       "6099  6100  0.082555   0.008261  0.008399  0.557220  0.026220  0.317345\n",
       "\n",
       "[6100 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"DACON.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline은 참가자의 제출을 최우선 목표로 하고 있습니다.  \n",
    "창의적인 전처리 방법을 적용하고 훌륭한 분석 모델을 개발해 주세요  \n",
    "  \n",
    "감사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
